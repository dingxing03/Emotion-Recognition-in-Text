{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772e2751",
   "metadata": {},
   "source": [
    "<h1> Title: Graph Neural Network (GNN) for Emotion Correlation Refinement </h1>\n",
    "\n",
    "<strong>Overview: In this section, I aim to enhance the predictions from the pretrained DistilBERT model by modeling the correlations between emotions using Graph Neural Networks (GNNs).</strong><br>\n",
    "The GNN component acts as a refinement layer, leveraging emotion co-occurrence patterns to improve recognition of rare and correlated emotions.<br>\n",
    "\n",
    "In this section, it covers:<br>\n",
    "1.0 Load pretrained DistilBERT model, tokenizer, and label binarizer<br>\n",
    "2.0 Generate initial predictions (logits) for the training set<br>\n",
    "3.0 Build emotion correlation graph<br>\n",
    "4.0 Train GNN refiner<br>\n",
    "5.0 Train the GNN on the DistilBERT outputs<br>\n",
    "6.0 Evaluate GNN<br>\n",
    "7.0 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555104df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5ae39",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.0 Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43562524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your saved model and tokenizer\n",
    "model_dir = Path(\"model_directory/distilbert\")\n",
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained(str(model_dir / \"distilbert_model\"))\n",
    "distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained(str(model_dir / \"distilbert_tokenizer\"))\n",
    "label_binarizer = joblib.load(model_dir / \"label_binarizer.pkl\") \n",
    "metadata = joblib.load(model_dir / \"label_binarizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39036cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=28, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the model is in evaluation mode and on the correct device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "distilbert_model.to(device)\n",
    "distilbert_model.eval() # Crucial: this disables dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55391fe6",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.0 Generate initial predictions (Logits) for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad479066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GoEmotions dataset\n",
    "df = pd.read_csv('../Datasets/GoEmotions.csv', converters={'emotion': eval, 'vector': eval})\n",
    "emotion_columns = label_binarizer.classes_ # Get the list of emotion classes\n",
    "num_emotions = len(emotion_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236ba754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 124688 samples\n",
      "Validation set: 41563 samples\n",
      "Test set: 41563 samples\n"
     ]
    }
   ],
   "source": [
    "# Random splitting\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    range(len(df)), test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: 75% train, 25% validation (of the remaining 80%)\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_val_idx, test_size=0.25, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(train_idx)} samples\")\n",
    "print(f\"Validation set: {len(val_idx)} samples\") \n",
    "print(f\"Test set: {len(test_idx)} samples\")\n",
    "\n",
    "# Create split masks\n",
    "train_mask = torch.zeros(len(df), dtype=torch.bool)\n",
    "val_mask = torch.zeros(len(df), dtype=torch.bool)\n",
    "test_mask = torch.zeros(len(df), dtype=torch.bool)\n",
    "\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29571dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate multi-label accuracy (threshold match)\n",
    "def calculate_threshold_accuracy(y_true, y_pred_proba, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate accuracy where prediction is correct if all predicted emotions \n",
    "    match the true emotions exactly (threshold match accuracy)\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    correct = 0\n",
    "    total = len(y_true)\n",
    "    \n",
    "    for i in range(total):\n",
    "        true_emotions = set(np.where(y_true[i] == 1)[0])\n",
    "        pred_emotions = set(np.where(y_pred[i] == 1)[0])\n",
    "        if true_emotions == pred_emotions:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2902202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate per-label F1 and find optimal thresholds\n",
    "def find_optimal_thresholds(y_true, y_pred_proba, threshold_range=np.arange(0.1, 0.9, 0.05)):\n",
    "    \"\"\"\n",
    "    Find optimal threshold for each emotion label by maximizing F1 score\n",
    "    \"\"\"\n",
    "    optimal_thresholds = []\n",
    "    best_f1_scores = []\n",
    "    \n",
    "    for label_idx in range(y_true.shape[1]):\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.1\n",
    "        \n",
    "        for threshold in threshold_range:\n",
    "            y_pred = (y_pred_proba[:, label_idx] >= threshold).astype(int)\n",
    "            f1 = f1_score(y_true[:, label_idx], y_pred, zero_division=0)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        optimal_thresholds.append(best_threshold)\n",
    "        best_f1_scores.append(best_f1)\n",
    "    \n",
    "    return np.array(optimal_thresholds), np.array(best_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788159d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training texts \n",
    "encodings = distilbert_tokenizer(df['text'].tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Create a PyTorch Dataset and DataLoader\n",
    "dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b169d6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train logits shape: torch.Size([124688, 28])\n",
      "Validation logits shape: torch.Size([41563, 28])\n",
      "Test logits shape: torch.Size([41563, 28])\n"
     ]
    }
   ],
   "source": [
    "# Run inference and collect logits\n",
    "initial_logits_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = [b.to(device) for b in batch]\n",
    "        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        initial_logits_list.append(logits.cpu())\n",
    "\n",
    "initial_logits_tensor = torch.cat(initial_logits_list, dim=0)\n",
    "true_labels = np.array(df['vector'].tolist())\n",
    "true_labels_tensor = torch.tensor(true_labels, dtype=torch.float)\n",
    "\n",
    "# Split the logits and labels according to our splits\n",
    "train_logits = initial_logits_tensor[train_mask]\n",
    "val_logits = initial_logits_tensor[val_mask] \n",
    "test_logits = initial_logits_tensor[test_mask]\n",
    "\n",
    "train_labels = true_labels_tensor[train_mask]\n",
    "val_labels = true_labels_tensor[val_mask]\n",
    "test_labels = true_labels_tensor[test_mask]\n",
    "\n",
    "print(f\"Train logits shape: {train_logits.shape}\")\n",
    "print(f\"Validation logits shape: {val_logits.shape}\")\n",
    "print(f\"Test logits shape: {test_logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490cc78",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.0 Build the emotion correlation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33d072f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 696 edges\n",
      "Edge weight range: [1.000, 847.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21180\\3427803976.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  edge_index = torch.tensor([rows, cols], dtype=torch.long, device=device)\n"
     ]
    }
   ],
   "source": [
    "# Build emotion correlation graph from TRAIN labels only\n",
    "train_labels_array = train_labels.numpy()\n",
    "cooc = train_labels_array.T @ train_labels_array\n",
    "np.fill_diagonal(cooc, 0)\n",
    "\n",
    "rows, cols = np.nonzero(cooc)\n",
    "edge_index = torch.tensor([rows, cols], dtype=torch.long, device=device)\n",
    "edge_weight = torch.tensor(cooc[rows, cols], dtype=torch.float32, device=device)\n",
    "\n",
    "print(f\"Graph has {edge_index.shape[1]} edges\")\n",
    "print(f\"Edge weight range: [{edge_weight.min():.3f}, {edge_weight.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805f5e0",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.0 Train the GNN refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5e2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNRefiner(nn.Module):\n",
    "    def __init__(self, edge_index, edge_weight=None, hidden=64):\n",
    "        super().__init__()\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        self.gcn1 = GCNConv(1, hidden)\n",
    "        self.gcn2 = GCNConv(hidden, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, num_emotions) logits\n",
    "        outs = []\n",
    "        for i in range(x.size(0)):\n",
    "            xi = x[i].unsqueeze(1)  # (num_emotions, 1)\n",
    "            h = F.relu(self.gcn1(xi, self.edge_index, self.edge_weight))\n",
    "            h = self.dropout(h)\n",
    "            delta = self.gcn2(h, self.edge_index, self.edge_weight).squeeze(1)  # (num_emotions,)\n",
    "            outs.append(x[i] + 0.5 * delta)  # residual with small step\n",
    "        return torch.stack(outs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1368645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21180\\877066088.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_weight = torch.tensor(neg_counts / pos_counts, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the refiner with edge weights\n",
    "gnn_refiner = GNNRefiner(edge_index, edge_weight).to(device)\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "pos_counts = train_labels.sum(axis=0) + 1e-6\n",
    "neg_counts = train_labels.shape[0] - pos_counts + 1e-6\n",
    "pos_weight = torch.tensor(neg_counts / pos_counts, dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(gnn_refiner.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c03e55",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.0 Train the GNN on the DistilBERT outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9296953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_dataset = TensorDataset(train_logits, train_labels)\n",
    "val_dataset = TensorDataset(val_logits, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7be338ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/100\n",
      "  Train Loss: 2.1973, Val Loss: 1.7937\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 1/10\n",
      "Epoch 2/100\n",
      "  Train Loss: 1.8213, Val Loss: 1.7872\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 2/10\n",
      "Epoch 3/100\n",
      "  Train Loss: 1.8155, Val Loss: 1.7814\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 3/10\n",
      "Epoch 4/100\n",
      "  Train Loss: 1.8091, Val Loss: 1.7747\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 4/10\n",
      "Epoch 5/100\n",
      "  Train Loss: 1.8028, Val Loss: 1.7682\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 5/10\n",
      "Epoch 6/100\n",
      "  Train Loss: 1.7961, Val Loss: 1.7618\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 6/10\n",
      "Epoch 7/100\n",
      "  Train Loss: 1.7891, Val Loss: 1.7554\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 7/10\n",
      "Epoch 8/100\n",
      "  Train Loss: 1.7832, Val Loss: 1.7492\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 8/10\n",
      "Epoch 9/100\n",
      "  Train Loss: 1.7761, Val Loss: 1.7432\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 9/10\n",
      "Epoch 10/100\n",
      "  Train Loss: 1.7705, Val Loss: 1.7371\n",
      "  Val Threshold Accuracy: 0.0000\n",
      "  Patience: 10/10\n",
      "Early stopping at epoch 10\n"
     ]
    }
   ],
   "source": [
    "# Training with early stopping\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "best_val_accuracy = 0\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    gnn_refiner.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        initial_logits_batch, labels_batch = batch\n",
    "        initial_logits_batch, labels_batch = initial_logits_batch.to(device), labels_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        refined_logits = gnn_refiner(initial_logits_batch)\n",
    "        loss = criterion(refined_logits, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    gnn_refiner.eval()\n",
    "    val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            initial_logits_batch, labels_batch = batch\n",
    "            initial_logits_batch, labels_batch = initial_logits_batch.to(device), labels_batch.to(device)\n",
    "            \n",
    "            refined_logits = gnn_refiner(initial_logits_batch)\n",
    "            loss = criterion(refined_logits, labels_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Get predictions for validation metrics\n",
    "            probs = torch.sigmoid(refined_logits).cpu().numpy()\n",
    "            val_predictions.append(probs)\n",
    "            val_true_labels.append(labels_batch.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_predictions = np.vstack(val_predictions)\n",
    "    val_true_labels = np.vstack(val_true_labels)\n",
    "    \n",
    "    # Use threshold 0.1 for validation (same as baseline)\n",
    "    val_accuracy = calculate_threshold_accuracy(val_true_labels, val_predictions, threshold=0.1)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    print(f'  Val Threshold Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "        best_model_state = gnn_refiner.state_dict().copy()\n",
    "        print(f'  New best validation accuracy: {best_val_accuracy:.4f}')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'  Patience: {patience_counter}/{patience}')\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    gnn_refiner.load_state_dict(best_model_state)\n",
    "    print(f'Loaded best model with validation accuracy: {best_val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628fc132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning thresholds on validation set...\n",
      "Optimal thresholds: [0.85 0.85 0.85 0.85 0.85 0.8  0.85 0.85 0.85 0.85 0.85 0.75 0.85 0.85\n",
      " 0.85 0.85 0.7  0.85 0.85 0.85 0.1  0.15 0.1  0.8  0.3  0.5  0.1  0.15]\n",
      "Best F1 scores: [0.3018757  0.40971039 0.35914894 0.20188671 0.17490909 0.29955947\n",
      " 0.23270135 0.22279229 0.25480769 0.18302215 0.26641144 0.27699531\n",
      " 0.25506377 0.18878005 0.41467676 0.75281772 0.19148936 0.31400137\n",
      " 0.57863661 0.24420024 0.08186999 0.02053643 0.07501681 0.01405622\n",
      " 0.00988875 0.1893269  0.0451904  0.42637631]\n",
      "Best global threshold: 0.850\n",
      "Best global accuracy: 0.0055\n"
     ]
    }
   ],
   "source": [
    "# Tune thresholds on validation set\n",
    "print(\"\\nTuning thresholds on validation set...\")\n",
    "gnn_refiner.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_refined_logits = gnn_refiner(val_logits.to(device))\n",
    "    val_probs = torch.sigmoid(val_refined_logits).cpu().numpy()\n",
    "\n",
    "# Find optimal per-label thresholds\n",
    "optimal_thresholds, best_f1_scores = find_optimal_thresholds(val_labels.numpy(), val_probs)\n",
    "print(f\"Optimal thresholds: {optimal_thresholds}\")\n",
    "print(f\"Best F1 scores: {best_f1_scores}\")\n",
    "\n",
    "# Also find optimal global threshold\n",
    "global_thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "global_accuracies = []\n",
    "\n",
    "for threshold in global_thresholds:\n",
    "    acc = calculate_threshold_accuracy(val_labels.numpy(), val_probs, threshold)\n",
    "    global_accuracies.append(acc)\n",
    "\n",
    "best_global_threshold = global_thresholds[np.argmax(global_accuracies)]\n",
    "best_global_accuracy = np.max(global_accuracies)\n",
    "\n",
    "print(f\"Best global threshold: {best_global_threshold:.3f}\")\n",
    "print(f\"Best global accuracy: {best_global_accuracy:.4f}\")\n",
    "\n",
    "# Save the optimal thresholds\n",
    "optimal_thresholds_dict = {\n",
    "    'per_label_thresholds': optimal_thresholds,\n",
    "    'global_threshold': best_global_threshold,\n",
    "    'validation_accuracy': best_val_accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5dbac",
   "metadata": {},
   "source": [
    "---\n",
    "# 6.0 Evaluate GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da8ae4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n",
      "Test Accuracy (Global threshold 0.850): 0.0056\n",
      "Baseline DistilBERT Test Accuracy (threshold 0.5): 0.0412\n"
     ]
    }
   ],
   "source": [
    "# Predict emotions for a list of texts using DistilBERT + GNN\n",
    "def predict_with_refiner(texts, use_per_label_thresholds=True):\n",
    "    # Tokenize and get DistilBERT initial logits\n",
    "    encodings = distilbert_tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = distilbert_model(**encodings)\n",
    "        initial_logits = outputs.logits\n",
    "\n",
    "    # Refine the logits using the trained GNN\n",
    "    with torch.no_grad():\n",
    "        gnn_refiner.eval()\n",
    "        refined_logits = gnn_refiner(initial_logits)\n",
    "\n",
    "    # Apply sigmoid to get probabilities\n",
    "    probs = torch.sigmoid(refined_logits).cpu().numpy()\n",
    "    \n",
    "    if use_per_label_thresholds:\n",
    "        # Use per-label thresholds\n",
    "        predictions = (probs >= optimal_thresholds).astype(int)\n",
    "    else:\n",
    "        # Use global threshold\n",
    "        predictions = (probs >= best_global_threshold).astype(int)\n",
    "    \n",
    "    predicted_emotions = label_binarizer.inverse_transform(predictions)\n",
    "    return predicted_emotions, probs\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "gnn_refiner.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_refined_logits = gnn_refiner(test_logits.to(device))\n",
    "    test_probs = torch.sigmoid(test_refined_logits).cpu().numpy()\n",
    "\n",
    "# Calculate test accuracy with different thresholds\n",
    "test_accuracy_global = calculate_threshold_accuracy(test_labels.numpy(), test_probs, best_global_threshold)\n",
    "test_accuracy_per_label = calculate_threshold_accuracy(test_labels.numpy(), test_probs, optimal_thresholds)\n",
    "\n",
    "print(f\"Test Accuracy (Global threshold {best_global_threshold:.3f}): {test_accuracy_global:.4f}\")\n",
    "\n",
    "# Compare with baseline DistilBERT performance\n",
    "test_baseline_probs = torch.sigmoid(test_logits).cpu().numpy()\n",
    "test_baseline_accuracy = calculate_threshold_accuracy(test_labels.numpy(), test_baseline_probs, 0.1)\n",
    "\n",
    "print(f\"Baseline DistilBERT Test Accuracy (threshold 0.5): {test_baseline_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864c8e2",
   "metadata": {},
   "source": [
    "---\n",
    "# 7.0 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f28e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GNN directory\n",
    "gnn_path = \"model_directory/gnn/\"\n",
    "os.makedirs(gnn_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf5c2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_directory/gnn/metadata.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model's state_dict\n",
    "torch.save(gnn_refiner.state_dict(), os.path.join(gnn_path, \"gnn_state_dict.pth\"))\n",
    "\n",
    "# Save the edge_index (emotion graph)\n",
    "torch.save(edge_index, os.path.join(gnn_path, \"edge_index.pt\"))\n",
    "torch.save(edge_weight, os.path.join(gnn_path, \"edge_weight.pt\"))\n",
    "\n",
    "# Save the label binarizer\n",
    "joblib.dump(label_binarizer, os.path.join(gnn_path, \"label_binarizer.pkl\"))\n",
    "\n",
    "# Save other metadata including optimal thresholds\n",
    "metadata = {\n",
    "    'optimal_thresholds': optimal_thresholds_dict,\n",
    "    'emotion_classes': label_binarizer.classes_.tolist(),\n",
    "    'num_emotions': num_emotions,\n",
    "    'best_validation_accuracy': best_val_accuracy,\n",
    "    'test_accuracy_global': test_accuracy_global,\n",
    "    'test_accuracy_per_label': test_accuracy_per_label,\n",
    "    'baseline_accuracy': test_baseline_accuracy\n",
    "}\n",
    "joblib.dump(metadata, os.path.join(gnn_path, \"metadata.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
